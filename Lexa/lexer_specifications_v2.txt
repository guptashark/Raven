From chapter 2.6: 

Read characters from input and group into token objects. 
Needs terminal symbol for parsing decisions... 

Tokens have attribute values. 
A token is a terminal with additional info. 

Sequence of input chars that make a token is the lexeme. 

We have sequences of regular exprs to identify what that 
lexeme fits as. If a particular pattern is matched, then 
the corresponding token name is given to that lexeme. 

/////////

2.6.1 Removal of Whitespace and comments 
	Some languages need whitespace, but that is rare. 
	Make a design desicion as to wether whitespace info should
	be included. The point is to remove useless characters from 
	the input and not pass them on to the parser. (The less work
	that the parser has to do, the better) 

2.6.2 Reading Ahead
	Some strings require us to read ahead to determine the
	token name, or correct lexeme. Ex: < and <=. Can't 
	accept <, need to wait to see if it's <=. 

	We use the input buffer, so that we don't have to fetch
	one character at a time, and make ridiculous numbers
	of IO calls. usually use a pointer to the character in 
	the buffer that gets pushed forward and backward. 

	Implementation hint: can potentially use a variable 
	"peek" to store next char in buffer, considering 
	we usually only have to look past one character. 

2.6.3 Constants
	We want to treat numbers as just "numbers" during
	parsing, so the lexer is responsible for turning
	strings of characters that make up numbers into 
	number constants. This means the lexer needs the 
	regex patterns to understand the different possible
	representiations of numbers, (binary, hex, oct, etc) 

	Special processing is needed. 

2.6.4 Keywords and identifiers
	There are set words that must be identified as keywords
	There are also rules for identifiers that can name
	things like arrays, variables, class names, etc. 

	Main issue is that most keywords satisfy rules for 
	identifiers so you need to know if an id is a keyword
	with some post processing. Usually a dictionary lookup. 

	IMPORTANT: In the symbol table, it's best to work with
	pointers to the strings instead of the strings themselves. 

	Reserved words are searched for either by initializing 
	a symbol table with reserved words, or having some other
	kind of lookup. 

	If you can't find an id in the reserved words table, 
	then you install it into the symbol table and go from 
	there. 

2.6.5 A lexical Analyzer
	Since the tokens might be holding different things, 
	ie, a num constant holds a number, an id token holds
	a pointer to something in the symbol table, we can 
	set it up so that there are token subclasses that
	have the appropriate fields. 
	
	The book uses an enum to describe the different
	tags.Â‰
	
	
	The scanner itself checks to see if it's got a 
	digit. If yes, it tries to match a number and 
	then return. (specific hardcoded rule) 
	
	if it is able to match an ID, it tries to do so, 
	and looks it up to see if it is a keyword. If 
	not, then it installs it into the dict. 
	
2.6.6 Exercises: 
	1) 	Need to be able to take care of comments: 
		// to end of line
		/* to */ multiline
		(Basic C style stuff) 	

	2) 	Extension to understand relational operators

	3) 	Extension to understand floating point nums

Observations: 
	While this does work as a lexlical analyzer, it
	leaves things to be desired. Limitations: 
	
	* hard to add new patterns
	* Hard to modify patterns (ie, what an ID should look like) 
	* Not very modular, it's one massive function, unless we
		want to start passing a pointer to the input stream around
		to small functions that each process a certain type of lexeme. 

**************************

Symbol Tables

since the symbol table is closely related to 
the Lexer, it's worthwhile to read up on this before doing 
the specs for chapter three. We won't do the intermediate
code gen. 

Scoping: 
	In order to implement scopes, a new symbol table must
	be created for each scope. 

	The lexer, parser and semantic analyzer can all create
	and use symbol table entries. In our case, since the 
	parser is in a better position to understand the 
	program structure, it will be doing the table entry 
	creation. 

	scope is super important to get right because 
	different parts of the program use the same var
	names, for instance "i, j, k" in loops, current, 
	next, etc. 

	When we create symbol tables, we think about what
	level a symbol table is at. the table in question
	must be contained within another table, and as such
	have access to its variables. So, we can have the
	idea of stacked symbol tables. 

	Sometimes, there are other structs used, like a 
	hash table to help get faster lookups. 

	Work from the inside out. 
	If you're in a symbol table, then search the current
	table for the symbol, if you can't find it, keep 
	working your way out until you do. If it's nowhere, 
	that's an error. 

	The way to implement nested symbol tables would be 
	to have a symbol table point to the symbol table of
	the enclosing block. 

	Question: How do we get to a lower level table? 
	If we can only go out in scope, how we descend in 
	scope? Do we ever need to? Is it made and destroyed
	on the fly? 

	It may be worth our time to read more about symbol
	tables before implementing one, or just rush in
	with what we think will work. Either way, we don't
	yet know enough to confidently create a symbol table. 



	


	
